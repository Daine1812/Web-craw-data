import firebase_admin
from firebase_admin import credentials, firestore
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import re
import time

# Khá»Ÿi táº¡o Firebase Admin SDK
cred = credentials.Certificate("firebase_service_account.json")  # Äáº£m báº£o tá»‡p .json náº±m cÃ¹ng thÆ° má»¥c vá»›i file Python
firebase_admin.initialize_app(cred)

# Khá»Ÿi táº¡o Firestore client
db = firestore.client()

# ÄÆ°á»ng dáº«n tá»›i ChromeDriver
CHROMEDRIVER_PATH = r'F:\Web Crawl data\chromedriver.exe'  # Äáº£m báº£o Ä‘Æ°á»ng dáº«n chÃ­nh xÃ¡c Ä‘áº¿n chromedriver cá»§a báº¡n

# Táº¡o driver Chrome
def create_driver():
    options = Options()
    options.add_argument("--headless")  # Cháº¡y á»Ÿ cháº¿ Ä‘á»™ áº©n
    options.add_argument("--disable-gpu")
    options.add_argument("--log-level=3")
    return webdriver.Chrome(service=Service(CHROMEDRIVER_PATH), options=options)

# HÃ m lá»c sá»‘ Ä‘iá»‡n thoáº¡i (báº¯t Ä‘áº§u tá»« sá»‘ 0)
def extract_phone_number(text):
    phone_pattern = re.compile(r'\b0\d{1,2}[-.\s]?\d{3}[-.\s]?\d{3,4}\b')
    phone_numbers = phone_pattern.findall(text)
    return phone_numbers if phone_numbers else None

# HÃ m lÆ°u dá»¯ liá»‡u vÃ o Firebase Firestore
def save_to_firebase(data):
    doc_ref = db.collection("web_data").add(data)
    print("\nğŸ“¤ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Firebase.")

# HÃ m láº¥y thÃ´ng tin trang web
def auto_extract_content(url):
    driver = create_driver()
    driver.get(url)
    time.sleep(3)  # Chá» má»™t chÃºt cho trang web táº£i xong

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    # 1. Láº¥y tiÃªu Ä‘á» trang
    title = soup.title.get_text(strip=True) if soup.title else "KhÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á»"
    print(f"\nğŸ¯ TiÃªu Ä‘á» trang: {title}")

    # 2. Láº¥y táº¥t cáº£ cÃ¡c link (URLs) trÃªn trang
    links = soup.find_all("a", href=True)
    link_list = [link['href'] for link in links]

    # 3. Lá»c ná»™i dung chÃ­nh cá»§a trang (thÆ°á»ng lÃ  cÃ¡c article, section, main, div, p...)
    main_content = soup.find_all(["article", "section", "main", "div", "p"])
    content_list = []
    for block in main_content:
        text = block.get_text(strip=True, separator=" ")
        if text and len(text) > 100:  # Chá»‰ láº¥y cÃ¡c Ä‘oáº¡n vÄƒn dÃ i hÆ¡n 100 kÃ½ tá»±
            content_list.append(text)

    # 4. Lá»c sá»‘ Ä‘iá»‡n thoáº¡i náº¿u cÃ³
    phone_numbers = extract_phone_number(soup.get_text())

    # 5. XÃ¡c Ä‘á»‹nh loáº¡i trang web (VÃ­ dá»¥: bÃ¡n Ä‘iá»‡n thoáº¡i, dá»‹ch vá»¥, tin tá»©c...)
    web_type = "ChÆ°a rÃµ loáº¡i"
    if "phone" in title.lower() or "mobile" in title.lower():
        web_type = "Trang web bÃ¡n Ä‘iá»‡n thoáº¡i"
    elif "shop" in title.lower() or "store" in title.lower():
        web_type = "Trang web mua sáº¯m"
    elif "news" in title.lower():
        web_type = "Trang web tin tá»©c"

    # Táº¡o dá»¯ liá»‡u Ä‘á»ƒ lÆ°u vÃ o Firebase
    data = {
        "title": title,
        "url": url,
        "links": link_list,
        "main_content": content_list,
        "phone_numbers": phone_numbers if phone_numbers else None,
        "web_type": web_type
    }

    # LÆ°u dá»¯ liá»‡u vÃ o Firebase
    save_to_firebase(data)

if __name__ == "__main__":
    url = input("ğŸŒ Nháº­p URL trang web: ")
    auto_extract_content(url)
